============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.2, pluggy-1.5.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /home/bitsandbytes
configfile: pytest.ini
collecting ... collected 88 items

tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_bnb_4bit_wrong_config PASSED [  1%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_device_and_dtype_assignment 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
FAILED                                                                   [  2%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_device_and_dtype_assignment ERROR [  2%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_fp32_4bit_conversion PASSED [  3%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_generate_quality PASSED [  4%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_generate_quality_config PASSED [  5%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_generate_quality_dequantize PASSED [  6%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_linear_are_4bit PASSED [  7%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_memory_footprint PASSED [  9%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_original_dtype PASSED [ 10%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_quantization_config_json_serialization PASSED [ 11%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_quantization_num_parameters PASSED [ 12%]
tests/transformer_tests/test_4bit.py::Bnb4BitTest::test_rwkv_4bit PASSED [ 13%]
tests/transformer_tests/test_4bit.py::Bnb4BitT5Test::test_inference_with_keep_in_fp32 PASSED [ 14%]
tests/transformer_tests/test_4bit.py::Bnb4BitT5Test::test_inference_without_keep_in_fp32 PASSED [ 15%]
tests/transformer_tests/test_4bit.py::Classes4BitModelTest::test_correct_head_class PASSED [ 17%]
tests/transformer_tests/test_4bit.py::Pipeline4BitTest::test_pipeline PASSED [ 18%]
tests/transformer_tests/test_4bit.py::Bnb4bitTestMultiGpu::test_multi_gpu_loading FAILED [ 19%]
tests/transformer_tests/test_4bit.py::Bnb4bitTestMultiGpu::test_multi_gpu_loading ERROR [ 19%]
tests/transformer_tests/test_4bit.py::Bnb4BitTestTraining::test_training PASSED [ 20%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_bnb_4bit_wrong_config PASSED [ 21%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_device_and_dtype_assignment 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
PASSED                                                                   [ 22%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_fp32_4bit_conversion PASSED [ 23%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality PASSED [ 25%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality_config PASSED [ 26%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality_dequantize PASSED [ 27%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_linear_are_4bit PASSED [ 28%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_memory_footprint PASSED [ 29%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_original_dtype PASSED [ 30%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_quantization_config_json_serialization PASSED [ 31%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_quantization_num_parameters PASSED [ 32%]
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_rwkv_4bit PASSED [ 34%]
tests/transformer_tests/test_4bit.py::BaseSerializationTest::test_serialization FAILED [ 35%]
tests/transformer_tests/test_4bit.py::BaseSerializationTest::test_serialization ERROR [ 35%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_double_safe FAILED [ 36%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_double_safe ERROR [ 36%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_double_unsafe FAILED [ 37%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_double_unsafe ERROR [ 37%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_single_safe FAILED [ 38%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_single_safe ERROR [ 38%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_single_unsafe FAILED [ 39%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_fp4_single_unsafe ERROR [ 39%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_nf4_double_unsafe FAILED [ 40%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_nf4_double_unsafe ERROR [ 40%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_nf4_single_safe FAILED [ 42%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_nf4_single_safe ERROR [ 42%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_nf4_single_unsafe FAILED [ 43%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_nf4_single_unsafe ERROR [ 43%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_serialization FAILED [ 44%]
tests/transformer_tests/test_4bit.py::ExtendedSerializationTest::test_serialization ERROR [ 44%]
tests/transformer_tests/test_4bit.py::BloomSerializationTest::test_serialization FAILED [ 45%]
tests/transformer_tests/test_4bit.py::BloomSerializationTest::test_serialization ERROR [ 45%]
tests/transformer_tests/test_4bit.py::GPTSerializationTest::test_serialization FAILED [ 46%]
tests/transformer_tests/test_4bit.py::GPTSerializationTest::test_serialization ERROR [ 46%]
tests/transformer_tests/test_4bit.py::Bnb4BitTestBasicConfigTest::test_load_in_4_and_8_bit_fails PASSED [ 47%]
tests/transformer_tests/test_4bit.py::Bnb4BitTestBasicConfigTest::test_set_load_in_8_bit PASSED [ 48%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_device_and_dtype_assignment 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
FAILED                                                                   [ 50%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_device_and_dtype_assignment ERROR [ 50%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_fp32_int8_conversion PASSED [ 51%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_generate_quality FAILED [ 52%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_generate_quality ERROR [ 52%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_generate_quality_config FAILED [ 53%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_generate_quality_config ERROR [ 53%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_generate_quality_dequantize PASSED [ 54%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert PASSED [ 55%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code 
-------------------------------- live log call ---------------------------------
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:234 Instantiating an MPTForCausalLM model from /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/modeling_mpt.py
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:66 We recommend using config.init_device="meta" with Composer + FSDP for faster initialization.
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:234 Instantiating an MPTForCausalLM model from /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/modeling_mpt.py
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:66 We recommend using config.init_device="meta" with Composer + FSDP for faster initialization.
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
PASSED                                                                   [ 56%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_from_pretrained FAILED [ 57%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_from_pretrained ERROR [ 57%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization FAILED [ 59%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization ERROR [ 59%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_regression FAILED [ 60%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_regression ERROR [ 60%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_sharded FAILED [ 61%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_sharded ERROR [ 61%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_linear_are_8bit PASSED [ 62%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_llm_skip PASSED [ 63%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_memory_footprint PASSED [ 64%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_original_dtype PASSED [ 65%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_quantization_config_json_serialization PASSED [ 67%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_raise_if_config_and_load_in_8bit PASSED [ 68%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32 FAILED [ 69%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32 ERROR [ 69%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32_serialized FAILED [ 70%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32_serialized ERROR [ 70%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8T5Test::test_inference_without_keep_in_fp32 FAILED [ 71%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8T5Test::test_inference_without_keep_in_fp32 ERROR [ 71%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8ModelClassesTest::test_correct_head_class PASSED [ 72%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestPipeline::test_pipeline FAILED [ 73%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestPipeline::test_pipeline ERROR [ 73%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestMultiGpu::test_multi_gpu_loading FAILED [ 75%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestMultiGpu::test_multi_gpu_loading ERROR [ 75%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
FAILED                                                                   [ 76%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map ERROR [ 76%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map_kwargs 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
FAILED                                                                   [ 77%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map_kwargs ERROR [ 77%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_custom_device_map 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
FAILED                                                                   [ 78%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_custom_device_map ERROR [ 78%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_random_device_map 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
FAILED                                                                   [ 79%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_random_device_map ERROR [ 79%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestTraining::test_training FAILED [ 80%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestTraining::test_training ERROR [ 80%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_device_and_dtype_assignment 
-------------------------------- live log call ---------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
PASSED                                                                   [ 81%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_fp32_int8_conversion FAILED [ 82%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_fp32_int8_conversion ERROR [ 82%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality FAILED [ 84%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality ERROR [ 84%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality_config FAILED [ 85%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality_config ERROR [ 85%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality_dequantize PASSED [ 86%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert PASSED [ 87%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code 
-------------------------------- live log call ---------------------------------
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:234 Instantiating an MPTForCausalLM model from /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/modeling_mpt.py
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:66 We recommend using config.init_device="meta" with Composer + FSDP for faster initialization.
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:234 Instantiating an MPTForCausalLM model from /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/modeling_mpt.py
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:66 We recommend using config.init_device="meta" with Composer + FSDP for faster initialization.
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
INFO     transformers_modules.mosaicml.mpt-7b.ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7.modeling_mpt:modeling_mpt.py:75 Removing bias (Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)) from LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True).
PASSED                                                                   [ 88%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_from_pretrained FAILED [ 89%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_from_pretrained ERROR [ 89%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization FAILED [ 90%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization ERROR [ 90%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_regression FAILED [ 92%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_regression ERROR [ 92%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_sharded FAILED [ 93%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_sharded ERROR [ 93%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_linear_are_8bit PASSED [ 94%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_llm_skip PASSED [ 95%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_memory_footprint PASSED [ 96%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_original_dtype PASSED [ 97%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_quantization_config_json_serialization PASSED [ 98%]
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_raise_if_config_and_load_in_8bit PASSED [100%]

==================================== ERRORS ====================================
______ ERROR at teardown of Bnb4BitTest.test_device_and_dtype_assignment _______

self = <test_4bit.Bnb4BitTest testMethod=test_device_and_dtype_assignment>

    def test_device_and_dtype_assignment(self):
        r"""
        Test whether trying to cast (or assigning a device to) a model after converting it in 8-bit will throw an error.
        Checks also if other models are casted correctly.
        """
        with self.assertRaises(ValueError):
            # Tries with `str`
            self.model_4bit.to("cpu")
    
        with self.assertRaises(ValueError):
            # Tries with a `dtype``
            self.model_4bit.to(torch.float16)
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_4bit.to(torch.device("cuda:0"))
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_4bit.float()
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_4bit.half()
    
        # Test if we did not break anything
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
    
        self.model_fp16 = self.model_fp16.to(torch.float32)
>       _ = self.model_fp16.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_4bit.py:288: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:660: in forward
    inputs_embeds = self.word_embeddings(input_ids)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py:164: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[59414,  2670,  4040,   632]], device='cuda:0')
weight = Parameter containing:
tensor([[ 6.0310e-03, -1.2779e-03, -6.3667e-03,  ..., -8.1940e-03,
         -1.3077e-02,  9.6817e-03],
        [-1.9562e-02, -7.2098e-03,  8.4839e-03,  ..., -8.1253e-03,
         -9.6741e-03, -8.9798e-03],
        [ 1.7212e-02,  8.0466e-06, -9.1400e-03,  ..., -2.0782e-02,
          1.9577e-02,  1.9226e-02],
        ...,
        [ 6.8283e-04,  5.4026e-04,  1.9741e-03,  ...,  1.3485e-03,
         -4.8876e-05, -8.6260e-04],
        [ 6.8283e-04,  5.4169e-04,  1.9684e-03,  ...,  1.3447e-03,
         -5.0306e-05, -8.5878e-04],
        [ 6.7663e-04,  5.4169e-04,  1.9703e-03,  ...,  1.3475e-03,
         -5.2929e-05, -8.6451e-04]], device='cuda:15', requires_grad=True)
padding_idx = -1, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        .. note::
            Note that the analytical gradients of this function with respect to
            entries in :attr:`weight` at the row specified by :attr:`padding_idx`
            are expected to differ from the numerical ones.
    
        .. note::
            Note that `:class:`torch.nn.Embedding` differs from this function in
            that it initializes the row of :attr:`weight` specified by
            :attr:`padding_idx` to all zeros on construction.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
              where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0, 2, 0, 5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(
                embedding,
                (input, weight),
                input,
                weight,
                padding_idx=padding_idx,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                sparse=sparse,
            )
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:15 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)

/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2267: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
_______ ERROR at teardown of Bnb4bitTestMultiGpu.test_multi_gpu_loading ________

self = <test_4bit.Bnb4bitTestMultiGpu testMethod=test_multi_gpu_loading>

    def test_multi_gpu_loading(self):
        r"""
        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.
        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice
        """
    
        model_parallel = AutoModelForCausalLM.from_pretrained(
            self.model_name, load_in_4bit=True, device_map="balanced"
        )
    
        # Check correct device map
>       self.assertEqual(set(model_parallel.hf_device_map.values()), {0, 1})
E       AssertionError: Items in the first set but not the second:
E       15
E       Items in the second set but not the first:
E       0
E       1

tests/transformer_tests/test_4bit.py:482: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
________ ERROR at teardown of BaseSerializationTest.test_serialization _________

self = <test_4bit.BaseSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_____ ERROR at teardown of ExtendedSerializationTest.test_fp4_double_safe ______

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_double_safe>

    def test_fp4_double_safe(self):
>       self.test_serialization(quant_type="fp4", double_quant=True, safe_serialization=True)

tests/transformer_tests/test_4bit.py:658: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
____ ERROR at teardown of ExtendedSerializationTest.test_fp4_double_unsafe _____

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_double_unsafe>

    def test_fp4_double_unsafe(self):
>       self.test_serialization(quant_type="fp4", double_quant=True, safe_serialization=False)

tests/transformer_tests/test_4bit.py:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_____ ERROR at teardown of ExtendedSerializationTest.test_fp4_single_safe ______

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_single_safe>

    def test_fp4_single_safe(self):
>       self.test_serialization(quant_type="fp4", double_quant=False, safe_serialization=True)

tests/transformer_tests/test_4bit.py:652: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
____ ERROR at teardown of ExtendedSerializationTest.test_fp4_single_unsafe _____

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_single_unsafe>

    def test_fp4_single_unsafe(self):
>       self.test_serialization(quant_type="fp4", double_quant=False, safe_serialization=False)

tests/transformer_tests/test_4bit.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
____ ERROR at teardown of ExtendedSerializationTest.test_nf4_double_unsafe _____

self = <test_4bit.ExtendedSerializationTest testMethod=test_nf4_double_unsafe>

    def test_nf4_double_unsafe(self):
>       self.test_serialization(quant_type="nf4", double_quant=True, safe_serialization=False)

tests/transformer_tests/test_4bit.py:644: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_____ ERROR at teardown of ExtendedSerializationTest.test_nf4_single_safe ______

self = <test_4bit.ExtendedSerializationTest testMethod=test_nf4_single_safe>

    def test_nf4_single_safe(self):
>       self.test_serialization(quant_type="nf4", double_quant=False, safe_serialization=True)

tests/transformer_tests/test_4bit.py:641: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
____ ERROR at teardown of ExtendedSerializationTest.test_nf4_single_unsafe _____

self = <test_4bit.ExtendedSerializationTest testMethod=test_nf4_single_unsafe>

    def test_nf4_single_unsafe(self):
>       self.test_serialization(quant_type="nf4", double_quant=False, safe_serialization=False)

tests/transformer_tests/test_4bit.py:638: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
______ ERROR at teardown of ExtendedSerializationTest.test_serialization _______

self = <test_4bit.ExtendedSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
________ ERROR at teardown of BloomSerializationTest.test_serialization ________

self = <test_4bit.BloomSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 43],
        [ 50],
        [197],
        ...,
        [148],
        [117],
        [ 35]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_________ ERROR at teardown of GPTSerializationTest.test_serialization _________

self = <test_4bit.GPTSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 87],
        [234],
        [ 84],
        ...,
        [201],
        [135],
        [158]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_____ ERROR at teardown of MixedInt8Test.test_device_and_dtype_assignment ______

self = <test_mixed_int8.MixedInt8Test testMethod=test_device_and_dtype_assignment>

    def test_device_and_dtype_assignment(self):
        r"""
        Test whether trying to cast (or assigning a device to) a model after converting it in 8-bit will throw an error.
        Checks also if other models are casted correctly.
        """
        with self.assertRaises(ValueError):
            # Tries with `str`
            self.model_8bit.to("cpu")
    
        with self.assertRaises(ValueError):
            # Tries with a `dtype``
            self.model_8bit.to(torch.float16)
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_8bit.to(torch.device("cuda:0"))
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_8bit.float()
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_8bit.half()
    
        # Test if we did not break anything
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
    
        self.model_fp16 = self.model_fp16.to(torch.float32)
>       _ = self.model_fp16.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:349: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:660: in forward
    inputs_embeds = self.word_embeddings(input_ids)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py:164: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[59414,  2670,  4040,   632]], device='cuda:0')
weight = Parameter containing:
tensor([[ 6.0310e-03, -1.2779e-03, -6.3667e-03,  ..., -8.1940e-03,
         -1.3077e-02,  9.6817e-03],
        [-1.9562e-02, -7.2098e-03,  8.4839e-03,  ..., -8.1253e-03,
         -9.6741e-03, -8.9798e-03],
        [ 1.7212e-02,  8.0466e-06, -9.1400e-03,  ..., -2.0782e-02,
          1.9577e-02,  1.9226e-02],
        ...,
        [ 6.8283e-04,  5.4026e-04,  1.9741e-03,  ...,  1.3485e-03,
         -4.8876e-05, -8.6260e-04],
        [ 6.8283e-04,  5.4169e-04,  1.9684e-03,  ...,  1.3447e-03,
         -5.0306e-05, -8.5878e-04],
        [ 6.7663e-04,  5.4169e-04,  1.9703e-03,  ...,  1.3475e-03,
         -5.2929e-05, -8.6451e-04]], device='cuda:15', requires_grad=True)
padding_idx = -1, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        .. note::
            Note that the analytical gradients of this function with respect to
            entries in :attr:`weight` at the row specified by :attr:`padding_idx`
            are expected to differ from the numerical ones.
    
        .. note::
            Note that `:class:`torch.nn.Embedding` differs from this function in
            that it initializes the row of :attr:`weight` specified by
            :attr:`padding_idx` to all zeros on construction.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
              where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0, 2, 0, 5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(
                embedding,
                (input, weight),
                input,
                weight,
                padding_idx=padding_idx,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                sparse=sparse,
            )
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:15 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)

/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2267: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
___________ ERROR at teardown of MixedInt8Test.test_generate_quality ___________

self = <test_mixed_int8.MixedInt8Test testMethod=test_generate_quality>

    def test_generate_quality(self):
        r"""
        Test the generation quality of the quantized model and see that we are matching the expected output.
        Given that we are operating on small numbers + the testing model is relatively small, we might not get
        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.
        """
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = self.model_8bit.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_______ ERROR at teardown of MixedInt8Test.test_generate_quality_config ________

self = <test_mixed_int8.MixedInt8Test testMethod=test_generate_quality_config>

    def test_generate_quality_config(self):
        r"""
        Test that loading the model with the config is equivalent
        """
        bnb_config = BitsAndBytesConfig()
        bnb_config.load_in_8bit = True
    
        model_8bit_from_config = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit_from_config.generate(
            input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10
        )

tests/transformer_tests/test_mixed_int8.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________ ERROR at teardown of MixedInt8Test.test_int8_from_pretrained _________

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_from_pretrained>

    def test_int8_from_pretrained(self):
        r"""
        Test whether loading a 8bit model from the Hub works as expected
        """
        from bitsandbytes.nn import Int8Params
    
        model_id = "ybelkada/bloom-1b7-8bit"
    
        model = AutoModelForCausalLM.from_pretrained(model_id)
    
        linear = get_some_linear_layer(model)
        self.assertTrue(linear.weight.__class__ == Int8Params)
        self.assertTrue(hasattr(linear.weight, "SCB"))
    
        # generate
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_from_model_config', 'transformers_version']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_from_model_config', 'transformers_version']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
__________ ERROR at teardown of MixedInt8Test.test_int8_serialization __________

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization>

    def test_int8_serialization(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
____ ERROR at teardown of MixedInt8Test.test_int8_serialization_regression _____

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_regression>

    def test_int8_serialization_regression(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - using not safetensors
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, safe_serialization=False)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:413: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
______ ERROR at teardown of MixedInt8Test.test_int8_serialization_sharded ______

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_sharded>

    def test_int8_serialization_sharded(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - sharded version.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, max_shard_size="200MB")
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname)
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|        | 1/8 [00:00<00:03,  2.04it/s]Loading checkpoint shards:  25%|       | 2/8 [00:00<00:01,  3.04it/s]Loading checkpoint shards:  38%|      | 3/8 [00:00<00:01,  3.70it/s]Loading checkpoint shards:  50%|     | 4/8 [00:01<00:00,  4.10it/s]Loading checkpoint shards:  62%|   | 5/8 [00:01<00:00,  4.33it/s]Loading checkpoint shards:  75%|  | 6/8 [00:01<00:00,  4.49it/s]Loading checkpoint shards:  88%| | 7/8 [00:01<00:00,  4.65it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.95it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.22it/s]
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|        | 1/8 [00:00<00:03,  2.05it/s]Loading checkpoint shards:  25%|       | 2/8 [00:00<00:01,  3.07it/s]Loading checkpoint shards:  38%|      | 3/8 [00:00<00:01,  3.67it/s]Loading checkpoint shards:  50%|     | 4/8 [00:01<00:00,  4.04it/s]Loading checkpoint shards:  62%|   | 5/8 [00:01<00:00,  4.24it/s]Loading checkpoint shards:  75%|  | 6/8 [00:01<00:00,  4.46it/s]Loading checkpoint shards:  88%| | 7/8 [00:01<00:00,  4.60it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.90it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.18it/s]
____ ERROR at teardown of MixedInt8T5Test.test_inference_with_keep_in_fp32 _____

self = <test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32>

    def test_inference_with_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        import bitsandbytes as bnb
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
    
        # there was a bug with decoders - this test checks that it is fixed
        self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(0)
>       _ = model.generate(**encoded_input)

tests/transformer_tests/test_mixed_int8.py:525: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1739: in forward
    decoder_outputs = self.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  -1,    0,   -1,    0,   -1,    0,   -1,    1,    0,    0,    0,   -3,
            0,    0,   -9,    0,    0,    0,   -1,   -1,    0,    1,   -1,   -5,
            0,    0,    1,    1,    0,    0,    0,   -1,    0,    0,    1,   -1,
            0,    0,   -1,    1,    0,    0,   -2,    0,    0,    0,   22,   -1,
            1,    0,    0,    0,    1,   -1,    0,    0,    0,    1,    0,    0,
           -1,    1,    0,    0,    0,    0,    0,    0,   -2,    0,    1,    0,
            0,    0,    1,   -1,    0,    0,    0,    1,   -1,   -1,    0,    0,
            1,    1,    1,    1,    0,   -6,   -1,    1,    1,    0,    1,    0,
           -4,    1,    1,    0,    0,    0,    0,   -1,    0,    1,    0,    1,
            1,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,
            0,   -1,    0,  -14,   -1,   -2,   -1,    0,   -1,   -1,    0,    0,
            1,    0,    0,    1,    0,   -8,   -1,   -1,    0,   -1,    0,    0,
            0,    0,    0,   -1,    0,    6,    0,    0,    1,    0,    0,    0,
           -1,    0,   -1,    6,    0,    0,    0,    0,    1,   -1,    5,    0,
            0,    0,    1,    0,   -2,    0,    0,    0,    0,    1,    0,    0,
            0,    0,    0,    1,    0,   -1,    0,    0,    0,    0,   10,   -2,
            0,    2,    0,    0,    0,    0,    0,   -1,    0,   -1,    0,   -1,
            0,    1,    1,    0,   -1,    0,    0,   -1,    1,   -2,    0,    0,
            0,    1,    0,    1,    0,   -1,    1,   -1,   -1,    0,    0,    0,
            0,    1,    0,    1,   -1,    1,    0,    0,    1,    0,    0,    3,
            1,    0,    1,    1,    1,   -2,    0,    0,    3,    0,    1,    0,
            0,    0,    0,    0,    0,    1,   -1,    0,    1,    0,    0,   -3,
            0,    0,    0,    1,    1,    0,   -1,    0,    0,   -4,    0,   -1,
            0,    0,    1,    0,   -1,    0,    1,   -1,    0,    1,    0,    0,
            0,    0,    0,    0,    0,    0,    6,    2,    0,    0,    0,   -8,
            0,    0,    0,    1,   -1,    0,   -1,    0,   -6,    1,   -1,    0,
            7,    0,    0,    0,    0,    0,   19,    0,    0,    0,    0,    1,
            4,    0,   -1,    0,    0,   -1,    0,    1,   -1,    0,    0,    0,
            0,    1,    0,    0,    0,    1,    0,   -1,    0,   -1,    0,    0,
            0,    1,    0,    1,    0,    0,    0,    3,   -1,    0,    0,    0,
            0,    0,    0,    2,    0,    0,    1,   -2,   -1,    0,    0,    0,
            0,    0,   11,    0,    1,   -1,    0,    0,    0,    2,    0,   -1,
           -1,    0,   -2,   -1,   -1,    0,    1,    0,  -19,    0,    0,    0,
           -1,   -1,    0,   -1,    1,    1,    0,   -1,    1,    1,    0,    0,
            0,    0,    1,    0,    0,    2,    0,    0,   -1,    1,    0,    0,
           -1,    0,    0,   -1,    1,    0,    0,    0,    0,   -1,    0,    0,
            1,   -1,    0,    0,   -1,    0,    0,    0,   -1,   -1,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    4,    1,    0,    0,
            0,    0,    0,    0,    0,   -2,    0,    0,    0,    1,    0, -127,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,  -11,
            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,   -1,
           -1,    1,    0,    1,    0,    1,   -1,    0,    0,    1,    0,   -1,
            0,    1,   -1,    1,    0,    0,    1,   -1]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-14, -33,  37,  ...,   3,  30,   8],
        [ 34,  44,  56,  ...,  -1,  -4,  44],
        [-18,   3, -18,  ...,  -7,  -8,  15],
        ...,
        [ 12, -70,   9,  ...,  11,   9, -14],
        [-30,  14,   3,  ...,  -3, -11,  30],
        [ -2,  26, -28,  ...,  -3,  58, -24]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_ ERROR at teardown of MixedInt8T5Test.test_inference_with_keep_in_fp32_serialized _

self = <test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32_serialized>

    def test_inference_with_keep_in_fp32_serialized(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly on
        a serialized model.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        import bitsandbytes as bnb
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
    
        with tempfile.TemporaryDirectory() as tmp_dir:
            model.save_pretrained(tmp_dir)
    
            model = T5ForConditionalGeneration.from_pretrained(tmp_dir)
    
            # there was a bug with decoders - this test checks that it is fixed
            self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))
    
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(0)
>           _ = model.generate(**encoded_input)

tests/transformer_tests/test_mixed_int8.py:557: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1739: in forward
    decoder_outputs = self.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  -1,    0,   -1,    0,   -1,    0,   -1,    1,    0,    0,    0,   -3,
            0,    0,   -9,    0,    0,    0,   -1,   -1,    0,    1,   -1,   -5,
            0,    0,    1,    1,    0,    0,    0,   -1,    0,    0,    1,   -1,
            0,    0,   -1,    1,    0,    0,   -2,    0,    0,    0,   22,   -1,
            1,    0,    0,    0,    1,   -1,    0,    0,    0,    1,    0,    0,
           -1,    1,    0,    0,    0,    0,    0,    0,   -2,    0,    1,    0,
            0,    0,    1,   -1,    0,    0,    0,    1,   -1,   -1,    0,    0,
            1,    1,    1,    1,    0,   -6,   -1,    1,    1,    0,    1,    0,
           -4,    1,    1,    0,    0,    0,    0,   -1,    0,    1,    0,    1,
            1,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,
            0,   -1,    0,  -14,   -1,   -2,   -1,    0,   -1,   -1,    0,    0,
            1,    0,    0,    1,    0,   -8,   -1,   -1,    0,   -1,    0,    0,
            0,    0,    0,   -1,    0,    6,    0,    0,    1,    0,    0,    0,
           -1,    0,   -1,    6,    0,    0,    0,    0,    1,   -1,    5,    0,
            0,    0,    1,    0,   -2,    0,    0,    0,    0,    1,    0,    0,
            0,    0,    0,    1,    0,   -1,    0,    0,    0,    0,   10,   -2,
            0,    2,    0,    0,    0,    0,    0,   -1,    0,   -1,    0,   -1,
            0,    1,    1,    0,   -1,    0,    0,   -1,    1,   -2,    0,    0,
            0,    1,    0,    1,    0,   -1,    1,   -1,   -1,    0,    0,    0,
            0,    1,    0,    1,   -1,    1,    0,    0,    1,    0,    0,    3,
            1,    0,    1,    1,    1,   -2,    0,    0,    3,    0,    1,    0,
            0,    0,    0,    0,    0,    1,   -1,    0,    1,    0,    0,   -3,
            0,    0,    0,    1,    1,    0,   -1,    0,    0,   -4,    0,   -1,
            0,    0,    1,    0,   -1,    0,    1,   -1,    0,    1,    0,    0,
            0,    0,    0,    0,    0,    0,    6,    2,    0,    0,    0,   -8,
            0,    0,    0,    1,   -1,    0,   -1,    0,   -6,    1,   -1,    0,
            7,    0,    0,    0,    0,    0,   19,    0,    0,    0,    0,    1,
            4,    0,   -1,    0,    0,   -1,    0,    1,   -1,    0,    0,    0,
            0,    1,    0,    0,    0,    1,    0,   -1,    0,   -1,    0,    0,
            0,    1,    0,    1,    0,    0,    0,    3,   -1,    0,    0,    0,
            0,    0,    0,    2,    0,    0,    1,   -2,   -1,    0,    0,    0,
            0,    0,   11,    0,    1,   -1,    0,    0,    0,    2,    0,   -1,
           -1,    0,   -2,   -1,   -1,    0,    1,    0,  -19,    0,    0,    0,
           -1,   -1,    0,   -1,    1,    1,    0,   -1,    1,    1,    0,    0,
            0,    0,    1,    0,    0,    2,    0,    0,   -1,    1,    0,    0,
           -1,    0,    0,   -1,    1,    0,    0,    0,    0,   -1,    0,    0,
            1,   -1,    0,    0,   -1,    0,    0,    0,   -1,   -1,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    4,    1,    0,    0,
            0,    0,    0,    0,    0,   -2,    0,    0,    0,    1,    0, -127,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,  -11,
            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,   -1,
           -1,    1,    0,    1,    0,    1,   -1,    0,    0,    1,    0,   -1,
            0,    1,   -1,    1,    0,    0,    1,   -1]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-14, -33,  37,  ...,   3,  30,   8],
        [ 34,  44,  56,  ...,  -1,  -4,  44],
        [-18,   3, -18,  ...,  -7,  -8,  15],
        ...,
        [ 12, -70,   9,  ...,  11,   9, -14],
        [-30,  14,   3,  ...,  -3, -11,  30],
        [ -2,  26, -28,  ...,  -3,  58, -24]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
___ ERROR at teardown of MixedInt8T5Test.test_inference_without_keep_in_fp32 ___

self = <test_mixed_int8.MixedInt8T5Test testMethod=test_inference_without_keep_in_fp32>

    def test_inference_without_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        from transformers import T5ForConditionalGeneration
    
        modules = T5ForConditionalGeneration._keep_in_fp32_modules
        T5ForConditionalGeneration._keep_in_fp32_modules = None
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(0)
>       _ = model.generate(**encoded_input)

tests/transformer_tests/test_mixed_int8.py:498: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1739: in forward
    decoder_outputs = self.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  -1,    0,   -1,    0,   -1,    0,   -1,    1,    0,    0,    0,   -3,
            0,    0,   -9,    0,    0,    0,   -1,   -1,    0,    1,   -1,   -5,
            0,    0,    1,    1,    0,    0,    0,   -1,    0,    0,    1,   -1,
            0,    0,   -1,    1,    0,    0,   -2,    0,    0,    0,   22,   -1,
            1,    0,    0,    0,    1,   -1,    0,    0,    0,    1,    0,    0,
           -1,    1,    0,    0,    0,    0,    0,    0,   -2,    0,    1,    0,
            0,    0,    1,   -1,    0,    0,    0,    1,   -1,   -1,    0,    0,
            1,    1,    1,    1,    0,   -6,   -1,    1,    1,    0,    1,    0,
           -4,    1,    1,    0,    0,    0,    0,   -1,    0,    1,    0,    1,
            1,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,
            0,   -1,    0,  -14,   -1,   -2,   -1,    0,   -1,   -1,    0,    0,
            1,    0,    0,    1,    0,   -8,   -1,   -1,    0,   -1,    0,    0,
            0,    0,    0,   -1,    0,    6,    0,    0,    1,    0,    0,    0,
           -1,    0,   -1,    6,    0,    0,    0,    0,    1,   -1,    5,    0,
            0,    0,    1,    0,   -2,    0,    0,    0,    0,    1,    0,    0,
            0,    0,    0,    1,    0,   -1,    0,    0,    0,    0,   10,   -2,
            0,    2,    0,    0,    0,    0,    0,   -1,    0,   -1,    0,   -1,
            0,    1,    1,    0,   -1,    0,    0,   -1,    1,   -2,    0,    0,
            0,    1,    0,    1,    0,   -1,    1,   -1,   -1,    0,    0,    0,
            0,    1,    0,    1,   -1,    1,    0,    0,    1,    0,    0,    3,
            1,    0,    1,    1,    1,   -2,    0,    0,    3,    0,    1,    0,
            0,    0,    0,    0,    0,    1,   -1,    0,    1,    0,    0,   -3,
            0,    0,    0,    1,    1,    0,   -1,    0,    0,   -4,    0,   -1,
            0,    0,    1,    0,   -1,    0,    1,   -1,    0,    1,    0,    0,
            0,    0,    0,    0,    0,    0,    6,    2,    0,    0,    0,   -8,
            0,    0,    0,    1,   -1,    0,   -1,    0,   -6,    1,   -1,    0,
            7,    0,    0,    0,    0,    0,   19,    0,    0,    0,    0,    1,
            4,    0,   -1,    0,    0,   -1,    0,    1,   -1,    0,    0,    0,
            0,    1,    0,    0,    0,    1,    0,   -1,    0,   -1,    0,    0,
            0,    1,    0,    1,    0,    0,    0,    3,   -1,    0,    0,    0,
            0,    0,    0,    2,    0,    0,    1,   -2,   -1,    0,    0,    0,
            0,    0,   11,    0,    1,   -1,    0,    0,    0,    2,    0,   -1,
           -1,    0,   -2,   -1,   -1,    0,    1,    0,  -19,    0,    0,    0,
           -1,   -1,    0,   -1,    1,    1,    0,   -1,    1,    1,    0,    0,
            0,    0,    1,    0,    0,    2,    0,    0,   -1,    1,    0,    0,
           -1,    0,    0,   -1,    1,    0,    0,    0,    0,   -1,    0,    0,
            1,   -1,    0,    0,   -1,    0,    0,    0,   -1,   -1,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    4,    1,    0,    0,
            0,    0,    0,    0,    0,   -2,    0,    0,    0,    1,    0, -127,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,  -11,
            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,   -1,
           -1,    1,    0,    1,    0,    1,   -1,    0,    0,    1,    0,   -1,
            0,    1,   -1,    1,    0,    0,    1,   -1]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-14, -33,  37,  ...,   3,  30,   8],
        [ 34,  44,  56,  ...,  -1,  -4,  44],
        [-18,   3, -18,  ...,  -7,  -8,  15],
        ...,
        [ 12, -70,   9,  ...,  11,   9, -14],
        [-30,  14,   3,  ...,  -3, -11,  30],
        [ -2,  26, -28,  ...,  -3,  58, -24]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
___________ ERROR at teardown of MixedInt8TestPipeline.test_pipeline ___________

self = <test_mixed_int8.MixedInt8TestPipeline testMethod=test_pipeline>

    def test_pipeline(self):
        r"""
        The aim of this test is to verify that the mixed int8 is compatible with `pipeline` from transformers. Since
        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything
        on pipline.
        """
        # self._clear_cuda_cache()
        self.pipe = pipeline(
            "text-generation",
            model=self.model_name,
            model_kwargs={"device_map": "auto", "load_in_8bit": True},
            max_new_tokens=self.MAX_NEW_TOKENS,
        )
    
        # Real second forward pass
>       pipeline_output = self.pipe(self.input_text)

tests/transformer_tests/test_mixed_int8.py:646: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py:262: in __call__
    return super().__call__(text_inputs, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1254: in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1261: in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1161: in forward
    model_outputs = self._forward(model_inputs, **forward_params)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py:351: in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
______ ERROR at teardown of MixedInt8TestMultiGpu.test_multi_gpu_loading _______

self = <test_mixed_int8.MixedInt8TestMultiGpu testMethod=test_multi_gpu_loading>

    def test_multi_gpu_loading(self):
        r"""
        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.
        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice
        """
    
        model_parallel = AutoModelForCausalLM.from_pretrained(
            self.model_name, load_in_8bit=True, device_map="balanced"
        )
    
        # Check correct device map
>       self.assertEqual(set(model_parallel.hf_device_map.values()), {0, 1})
E       AssertionError: Items in the first set but not the second:
E       15
E       Items in the second set but not the first:
E       0
E       1

tests/transformer_tests/test_mixed_int8.py:666: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_ ERROR at teardown of MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map _

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map>

    def test_cpu_gpu_disk_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
            model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                quantization_config=bnb_config,
                offload_folder=tmpdirname,
            )
    
            # Check that the model has been correctly set on device 0, 1, and `cpu`.
            self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu", "disk"})
    
>           self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:792: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
_ ERROR at teardown of MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map_kwargs _

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map_kwargs>

    def test_cpu_gpu_disk_loading_custom_device_map_kwargs(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map - using the kwargs directly instead of the quantization config
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
            model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                offload_folder=tmpdirname,
            )
    
            # Check that the model has been correctly set on device 0, 1, and `cpu`.
            self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu", "disk"})
    
>           self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:819: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
_ ERROR at teardown of MixedInt8TestCpuGpu.test_cpu_gpu_loading_custom_device_map _

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_custom_device_map>

    def test_cpu_gpu_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time the device map is more organized than the test above and uses the abstraction
        `transformer.h` to encapsulate all the decoder layers.
        """
        device_map = {
            "transformer.word_embeddings": "cpu",
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": "cpu",
            "transformer.h": 0,
            "transformer.ln_f": 1,
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
        # Load model
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )
    
        # Check that the model has been correctly set on device 0, 1, and `cpu`.
        self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu"})
    
>       self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:765: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:0',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:0',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
_ ERROR at teardown of MixedInt8TestCpuGpu.test_cpu_gpu_loading_random_device_map _

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_random_device_map>

    def test_cpu_gpu_loading_random_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a random `device_map`.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": 0,
            "lm_head": 0,
            "transformer.h.0": "cpu",
            "transformer.h.1": "cpu",
            "transformer.h.2": 0,
            "transformer.h.3": 0,
            "transformer.h.4": 0,
            "transformer.h.5": 0,
            "transformer.h.6": 0,
            "transformer.h.7": 0,
            "transformer.h.8": 0,
            "transformer.h.9": 1,
            "transformer.h.10": 0,
            "transformer.h.11": 1,
            "transformer.h.12": 0,
            "transformer.h.13": 0,
            "transformer.h.14": 1,
            "transformer.h.15": 0,
            "transformer.h.16": 0,
            "transformer.h.17": 1,
            "transformer.h.18": 1,
            "transformer.h.19": 0,
            "transformer.h.20": 1,
            "transformer.h.21": 1,
            "transformer.h.22": 0,
            "transformer.h.23": 0,
            "transformer.ln_f": 1,
        }
    
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )
    
        # Check that the model has been correctly set on device 0, 1, and `cpu`.
        self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu"})
    
>       self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:738: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -6,  -6,  22,  ...,  21,  12,  -4],
        [ -1, -12,   7,  ...,  -9,  18,  15],
        [ -4,  -4, -15,  ...,   4, -23, -15],
        [ 19,  16,  20,  ...,  -7,  11,  -9]], device='cuda:0',
       dtype=torch.int8)
B = tensor([[ -8,  18,   7,  ..., -26,  -7, -43],
        [-20, -10, -11,  ...,  32, -53, -23],
        [ 30, -35, -45,  ..., -26, -52, -15],
        ...,
        [-88,   5, -13,  ...,  27, -15,  36],
        [-41,  21, -21,  ..., -20,  26, -16],
        [-36, -21,  24,  ..., -40, -26,  22]], device='cuda:0',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
___________ ERROR at teardown of MixedInt8TestTraining.test_training ___________

self = <test_mixed_int8.MixedInt8TestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version("bitsandbytes")) < version.parse("0.37.0"):
            self.skipTest(reason="This test requires bitsandbytes>=0.37.0")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)
    
        self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})
    
        for param in model.parameters():
            param.requires_grad = False  # freeze the model - train adapters later
            if param.ndim == 1:
                # cast the small parameters (e.g. layernorm) to fp32 for stability
                param.data = param.data.to(torch.float32)
    
        # Step 2: add adapters
        for _, module in model.named_modules():
            if "OPTAttention" in repr(type(module)):
                module.q_proj = LoRALayer(module.q_proj, rank=16)
                module.k_proj = LoRALayer(module.k_proj, rank=16)
                module.v_proj = LoRALayer(module.v_proj, rank=16)
    
        # Step 3: dummy batch
        batch = self.tokenizer("Test batch ", return_tensors="pt").to(0)
    
        # Step 4: Check if the gradient is not None
        with torch.cuda.amp.autocast():
>           out = model.forward(**batch)

tests/transformer_tests/test_mixed_int8.py:854: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py:1011: in forward
    outputs = self.model.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py:729: in forward
    inputs_embeds = self.project_in(inputs_embeds)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[-43, -30, -60,  ..., -22,  -4,  32],
        [ 65,   9,  45,  ...,  29,  66, -59],
        [-17, -17,  35,  ..., -11, -64, -55],
        [-23,   5, -40,  ..., -74, -11,  -8]], device='cuda:0',
       dtype=torch.int8)
B = tensor([[114, -71, -27,  ..., -73, -47,  66],
        [ 70,  95, -20,  ...,  15,  47, -15],
        [-86,  -4,   8,  ...,  62, -64, -10],
        ...,
        [ 87,  95,  55,  ...,  33,  16,  -9],
        [ 44,  11, -47,  ..., -43, -13, -47],
        [ 57, -15, -66,  ...,  42,  40, -64]], device='cuda:0',
       dtype=torch.int8)
SA = (torch.Size([4, 512]), 'col'), SB = (torch.Size([1024, 512]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)
Sout = ((4, 1024), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 512]), B: torch.Size([1024, 512]), C: (4, 1024); (lda, ldb, ldc): (c_int(4), c_int(1024), c_int(4)); (m, n, k): (c_int(4), c_int(1024), c_int(512))
A: torch.Size([4, 512]), B: torch.Size([1024, 512]), C: (4, 1024); (lda, ldb, ldc): (c_int(4), c_int(1024), c_int(4)); (m, n, k): (c_int(4), c_int(1024), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
_______ ERROR at teardown of MixedInt8GPT2Test.test_fp32_int8_conversion _______

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_fp32_int8_conversion>

    def test_fp32_int8_conversion(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        """
        model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small", load_in_8bit=True, device_map="auto")
>       self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.float32)
E       AssertionError: False is not true

tests/transformer_tests/test_mixed_int8.py:365: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________ ERROR at teardown of MixedInt8GPT2Test.test_generate_quality _________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality>

    def test_generate_quality(self):
        r"""
        Test the generation quality of the quantized model and see that we are matching the expected output.
        Given that we are operating on small numbers + the testing model is relatively small, we might not get
        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.
        """
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = self.model_8bit.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
_____ ERROR at teardown of MixedInt8GPT2Test.test_generate_quality_config ______

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality_config>

    def test_generate_quality_config(self):
        r"""
        Test that loading the model with the config is equivalent
        """
        bnb_config = BitsAndBytesConfig()
        bnb_config.load_in_8bit = True
    
        model_8bit_from_config = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit_from_config.generate(
            input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10
        )

tests/transformer_tests/test_mixed_int8.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
_______ ERROR at teardown of MixedInt8GPT2Test.test_int8_from_pretrained _______

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_from_pretrained>

    def test_int8_from_pretrained(self):
        r"""
        Test whether loading a 8bit model from the Hub works as expected
        """
        from bitsandbytes.nn import Int8Params
    
        model_id = "ybelkada/gpt2-xl-8bit"
    
        model = AutoModelForCausalLM.from_pretrained(model_id)
    
        linear = get_some_linear_layer(model)
        self.assertTrue(linear.weight.__class__ == Int8Params)
        self.assertTrue(hasattr(linear.weight, "SCB"))
    
        # generate
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:890: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
________ ERROR at teardown of MixedInt8GPT2Test.test_int8_serialization ________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization>

    def test_int8_serialization(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
__ ERROR at teardown of MixedInt8GPT2Test.test_int8_serialization_regression ___

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_regression>

    def test_int8_serialization_regression(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - using not safetensors
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, safe_serialization=False)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:413: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
____ ERROR at teardown of MixedInt8GPT2Test.test_int8_serialization_sharded ____

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_sharded>

    def test_int8_serialization_sharded(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - sharded version.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, max_shard_size="200MB")
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname)
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|         | 1/9 [00:00<00:01,  4.34it/s]Loading checkpoint shards:  22%|       | 2/9 [00:00<00:01,  4.22it/s]Loading checkpoint shards:  33%|      | 3/9 [00:00<00:01,  4.21it/s]Loading checkpoint shards:  44%|     | 4/9 [00:00<00:01,  4.20it/s]Loading checkpoint shards:  56%|    | 5/9 [00:01<00:00,  4.23it/s]Loading checkpoint shards:  67%|   | 6/9 [00:01<00:00,  4.25it/s]Loading checkpoint shards:  78%|  | 7/9 [00:01<00:00,  4.27it/s]Loading checkpoint shards:  89%| | 8/9 [00:01<00:00,  4.28it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.52it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.34it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|         | 1/9 [00:00<00:01,  4.27it/s]Loading checkpoint shards:  22%|       | 2/9 [00:00<00:01,  4.19it/s]Loading checkpoint shards:  33%|      | 3/9 [00:00<00:01,  4.20it/s]Loading checkpoint shards:  44%|     | 4/9 [00:00<00:01,  4.17it/s]Loading checkpoint shards:  56%|    | 5/9 [00:01<00:00,  4.17it/s]Loading checkpoint shards:  67%|   | 6/9 [00:01<00:00,  4.18it/s]Loading checkpoint shards:  78%|  | 7/9 [00:01<00:00,  4.15it/s]Loading checkpoint shards:  89%| | 8/9 [00:01<00:00,  4.10it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.34it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.23it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
=================================== FAILURES ===================================
_________________ Bnb4BitTest.test_device_and_dtype_assignment _________________

self = <test_4bit.Bnb4BitTest testMethod=test_device_and_dtype_assignment>

    def test_device_and_dtype_assignment(self):
        r"""
        Test whether trying to cast (or assigning a device to) a model after converting it in 8-bit will throw an error.
        Checks also if other models are casted correctly.
        """
        with self.assertRaises(ValueError):
            # Tries with `str`
            self.model_4bit.to("cpu")
    
        with self.assertRaises(ValueError):
            # Tries with a `dtype``
            self.model_4bit.to(torch.float16)
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_4bit.to(torch.device("cuda:0"))
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_4bit.float()
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_4bit.half()
    
        # Test if we did not break anything
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
    
        self.model_fp16 = self.model_fp16.to(torch.float32)
>       _ = self.model_fp16.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_4bit.py:288: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:660: in forward
    inputs_embeds = self.word_embeddings(input_ids)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py:164: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[59414,  2670,  4040,   632]], device='cuda:0')
weight = Parameter containing:
tensor([[ 6.0310e-03, -1.2779e-03, -6.3667e-03,  ..., -8.1940e-03,
         -1.3077e-02,  9.6817e-03],
        [-1.9562e-02, -7.2098e-03,  8.4839e-03,  ..., -8.1253e-03,
         -9.6741e-03, -8.9798e-03],
        [ 1.7212e-02,  8.0466e-06, -9.1400e-03,  ..., -2.0782e-02,
          1.9577e-02,  1.9226e-02],
        ...,
        [ 6.8283e-04,  5.4026e-04,  1.9741e-03,  ...,  1.3485e-03,
         -4.8876e-05, -8.6260e-04],
        [ 6.8283e-04,  5.4169e-04,  1.9684e-03,  ...,  1.3447e-03,
         -5.0306e-05, -8.5878e-04],
        [ 6.7663e-04,  5.4169e-04,  1.9703e-03,  ...,  1.3475e-03,
         -5.2929e-05, -8.6451e-04]], device='cuda:15', requires_grad=True)
padding_idx = -1, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        .. note::
            Note that the analytical gradients of this function with respect to
            entries in :attr:`weight` at the row specified by :attr:`padding_idx`
            are expected to differ from the numerical ones.
    
        .. note::
            Note that `:class:`torch.nn.Embedding` differs from this function in
            that it initializes the row of :attr:`weight` specified by
            :attr:`padding_idx` to all zeros on construction.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
              where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0, 2, 0, 5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(
                embedding,
                (input, weight),
                input,
                weight,
                padding_idx=padding_idx,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                sparse=sparse,
            )
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:15 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)

/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2267: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
__________________ Bnb4bitTestMultiGpu.test_multi_gpu_loading __________________

self = <test_4bit.Bnb4bitTestMultiGpu testMethod=test_multi_gpu_loading>

    def test_multi_gpu_loading(self):
        r"""
        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.
        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice
        """
    
        model_parallel = AutoModelForCausalLM.from_pretrained(
            self.model_name, load_in_4bit=True, device_map="balanced"
        )
    
        # Check correct device map
>       self.assertEqual(set(model_parallel.hf_device_map.values()), {0, 1})
E       AssertionError: Items in the first set but not the second:
E       15
E       Items in the second set but not the first:
E       0
E       1

tests/transformer_tests/test_4bit.py:482: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
___________________ BaseSerializationTest.test_serialization ___________________

self = <test_4bit.BaseSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
________________ ExtendedSerializationTest.test_fp4_double_safe ________________

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_double_safe>

    def test_fp4_double_safe(self):
>       self.test_serialization(quant_type="fp4", double_quant=True, safe_serialization=True)

tests/transformer_tests/test_4bit.py:658: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_______________ ExtendedSerializationTest.test_fp4_double_unsafe _______________

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_double_unsafe>

    def test_fp4_double_unsafe(self):
>       self.test_serialization(quant_type="fp4", double_quant=True, safe_serialization=False)

tests/transformer_tests/test_4bit.py:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
________________ ExtendedSerializationTest.test_fp4_single_safe ________________

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_single_safe>

    def test_fp4_single_safe(self):
>       self.test_serialization(quant_type="fp4", double_quant=False, safe_serialization=True)

tests/transformer_tests/test_4bit.py:652: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_______________ ExtendedSerializationTest.test_fp4_single_unsafe _______________

self = <test_4bit.ExtendedSerializationTest testMethod=test_fp4_single_unsafe>

    def test_fp4_single_unsafe(self):
>       self.test_serialization(quant_type="fp4", double_quant=False, safe_serialization=False)

tests/transformer_tests/test_4bit.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[113],
        [ 28],
        [ 97],
        ...,
        [103],
        [247],
        [166]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'fp4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_______________ ExtendedSerializationTest.test_nf4_double_unsafe _______________

self = <test_4bit.ExtendedSerializationTest testMethod=test_nf4_double_unsafe>

    def test_nf4_double_unsafe(self):
>       self.test_serialization(quant_type="nf4", double_quant=True, safe_serialization=False)

tests/transformer_tests/test_4bit.py:644: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
________________ ExtendedSerializationTest.test_nf4_single_safe ________________

self = <test_4bit.ExtendedSerializationTest testMethod=test_nf4_single_safe>

    def test_nf4_single_safe(self):
>       self.test_serialization(quant_type="nf4", double_quant=False, safe_serialization=True)

tests/transformer_tests/test_4bit.py:641: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_______________ ExtendedSerializationTest.test_nf4_single_unsafe _______________

self = <test_4bit.ExtendedSerializationTest testMethod=test_nf4_single_unsafe>

    def test_nf4_single_unsafe(self):
>       self.test_serialization(quant_type="nf4", double_quant=False, safe_serialization=False)

tests/transformer_tests/test_4bit.py:638: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_4bit.py:567: in test_serialization
    model_0 = AutoModelForCausalLM.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = False, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
_________________ ExtendedSerializationTest.test_serialization _________________

self = <test_4bit.ExtendedSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[167],
        [115],
        [152],
        ...,
        [138],
        [ 74],
        [ 25]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
__________________ BloomSerializationTest.test_serialization ___________________

self = <test_4bit.BloomSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 43],
        [ 50],
        [197],
        ...,
        [148],
        [117],
        [ 35]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
___________________ GPTSerializationTest.test_serialization ____________________

self = <test_4bit.GPTSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
>       model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )

tests/transformer_tests/test_4bit.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3990: in from_pretrained
    dispatch_model(model, **device_map_kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:420: in dispatch_model
    attach_align_device_hook_on_blocks(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:616: in attach_align_device_hook_on_blocks
    attach_execution_device_hook(module, execution_device[module_name], tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:438: in attach_execution_device_hook
    attach_execution_device_hook(child, execution_device, tied_params_map=tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:428: in attach_execution_device_hook
    add_hook_to_module(
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:160: in add_hook_to_module
    module = hook.init_hook(module)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:282: in init_hook
    set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:408: in set_module_tensor_to_device
    new_value = old_value.to(device)
bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 87],
        [234],
        [ 84],
        ...,
        [201],
        [135],
        [158]], device='cuda:15', dtype=torch.uint8)
absmax = tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:15')
out = tensor([[0],
        [0],
        [0],
        ...,
        [0],
        [0],
        [0]], device='cuda:15', dtype=torch.uint8)
blocksize = 128, compress_statistics = True, quant_type = 'nf4'
quant_storage = torch.uint8

    def quantize_4bit(
        self,
        A: torch.Tensor,
        absmax: Optional[torch.Tensor] = None,
        out: Optional[torch.Tensor] = None,
        blocksize: Optional[int] = None,
        compress_statistics=False,
        quant_type: Literal["fp4", "nf4"] = "fp4",
        quant_storage=torch.uint8,
    ) -> Tuple[torch.Tensor, QuantState]:
        if blocksize is None:
            # Some AMD GPUs have warpsize 64
            # Set default blocksize to 128 (~warpsize 64 in kernel) for HIP
            blocksize = 64 if not HIP_ENVIRONMENT else 128
        if A.device.type != "cuda":
            raise NotImplementedError(f"Device type not supported for FP4 quantization: {A.device.type}")
        if quant_type not in ["fp4", "nf4"]:
            raise NotImplementedError(f"4-bit quantization data type {quant_type} is not implemented.")
    
        n = A.numel()
        input_shape = A.shape
    
        if absmax is None:
            blocks = n // blocksize
            blocks += 1 if n % blocksize > 0 else 0
            absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
    
        if out is None:
            mod = dtype2bytes[quant_storage] * 2
            out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
    
        # Some AMD GPUs have warpsize 64
        # Set min blocksize to 128 (~warpsize 64 in kernel) for HIP
        if not HIP_ENVIRONMENT:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128, 64]
        else:
            assert blocksize in [4096, 2048, 1024, 512, 256, 128]
    
        prev_device = pre_call(A.device)
        is_on_gpu([A, out, absmax])
    
        if A.dtype == torch.float32:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp32_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp32_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.float16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_fp16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_fp16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        elif A.dtype == torch.bfloat16:
            if quant_type == "fp4":
                lib.cquantize_blockwise_bf16_fp4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
            else:
                lib.cquantize_blockwise_bf16_nf4(
                    get_ptr(None), get_ptr(A), get_ptr(absmax), get_ptr(out), ct.c_int32(blocksize), ct.c_int(n)
                )
    
        else:
>           raise ValueError(f"Blockwise quantization only supports 16/32-bit floats, but got {A.dtype}")
E           ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8

bitsandbytes/backends/cuda.py:517: ValueError
________________ MixedInt8Test.test_device_and_dtype_assignment ________________

self = <test_mixed_int8.MixedInt8Test testMethod=test_device_and_dtype_assignment>

    def test_device_and_dtype_assignment(self):
        r"""
        Test whether trying to cast (or assigning a device to) a model after converting it in 8-bit will throw an error.
        Checks also if other models are casted correctly.
        """
        with self.assertRaises(ValueError):
            # Tries with `str`
            self.model_8bit.to("cpu")
    
        with self.assertRaises(ValueError):
            # Tries with a `dtype``
            self.model_8bit.to(torch.float16)
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_8bit.to(torch.device("cuda:0"))
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_8bit.float()
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_8bit.half()
    
        # Test if we did not break anything
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
    
        self.model_fp16 = self.model_fp16.to(torch.float32)
>       _ = self.model_fp16.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:349: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:660: in forward
    inputs_embeds = self.word_embeddings(input_ids)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py:164: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[59414,  2670,  4040,   632]], device='cuda:0')
weight = Parameter containing:
tensor([[ 6.0310e-03, -1.2779e-03, -6.3667e-03,  ..., -8.1940e-03,
         -1.3077e-02,  9.6817e-03],
        [-1.9562e-02, -7.2098e-03,  8.4839e-03,  ..., -8.1253e-03,
         -9.6741e-03, -8.9798e-03],
        [ 1.7212e-02,  8.0466e-06, -9.1400e-03,  ..., -2.0782e-02,
          1.9577e-02,  1.9226e-02],
        ...,
        [ 6.8283e-04,  5.4026e-04,  1.9741e-03,  ...,  1.3485e-03,
         -4.8876e-05, -8.6260e-04],
        [ 6.8283e-04,  5.4169e-04,  1.9684e-03,  ...,  1.3447e-03,
         -5.0306e-05, -8.5878e-04],
        [ 6.7663e-04,  5.4169e-04,  1.9703e-03,  ...,  1.3475e-03,
         -5.2929e-05, -8.6451e-04]], device='cuda:15', requires_grad=True)
padding_idx = -1, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        .. note::
            Note that the analytical gradients of this function with respect to
            entries in :attr:`weight` at the row specified by :attr:`padding_idx`
            are expected to differ from the numerical ones.
    
        .. note::
            Note that `:class:`torch.nn.Embedding` differs from this function in
            that it initializes the row of :attr:`weight` specified by
            :attr:`padding_idx` to all zeros on construction.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
              where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0, 2, 0, 5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(
                embedding,
                (input, weight),
                input,
                weight,
                padding_idx=padding_idx,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                sparse=sparse,
            )
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:15 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)

/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2267: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
_____________________ MixedInt8Test.test_generate_quality ______________________

self = <test_mixed_int8.MixedInt8Test testMethod=test_generate_quality>

    def test_generate_quality(self):
        r"""
        Test the generation quality of the quantized model and see that we are matching the expected output.
        Given that we are operating on small numbers + the testing model is relatively small, we might not get
        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.
        """
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = self.model_8bit.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
__________________ MixedInt8Test.test_generate_quality_config __________________

self = <test_mixed_int8.MixedInt8Test testMethod=test_generate_quality_config>

    def test_generate_quality_config(self):
        r"""
        Test that loading the model with the config is equivalent
        """
        bnb_config = BitsAndBytesConfig()
        bnb_config.load_in_8bit = True
    
        model_8bit_from_config = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit_from_config.generate(
            input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10
        )

tests/transformer_tests/test_mixed_int8.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
___________________ MixedInt8Test.test_int8_from_pretrained ____________________

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_from_pretrained>

    def test_int8_from_pretrained(self):
        r"""
        Test whether loading a 8bit model from the Hub works as expected
        """
        from bitsandbytes.nn import Int8Params
    
        model_id = "ybelkada/bloom-1b7-8bit"
    
        model = AutoModelForCausalLM.from_pretrained(model_id)
    
        linear = get_some_linear_layer(model)
        self.assertTrue(linear.weight.__class__ == Int8Params)
        self.assertTrue(hasattr(linear.weight, "SCB"))
    
        # generate
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_from_model_config', 'transformers_version']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_from_model_config', 'transformers_version']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
____________________ MixedInt8Test.test_int8_serialization _____________________

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization>

    def test_int8_serialization(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
_______________ MixedInt8Test.test_int8_serialization_regression _______________

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_regression>

    def test_int8_serialization_regression(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - using not safetensors
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, safe_serialization=False)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:413: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
________________ MixedInt8Test.test_int8_serialization_sharded _________________

self = <test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_sharded>

    def test_int8_serialization_sharded(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - sharded version.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, max_shard_size="200MB")
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname)
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|        | 1/8 [00:00<00:03,  2.04it/s]Loading checkpoint shards:  25%|       | 2/8 [00:00<00:01,  3.04it/s]Loading checkpoint shards:  38%|      | 3/8 [00:00<00:01,  3.70it/s]Loading checkpoint shards:  50%|     | 4/8 [00:01<00:00,  4.10it/s]Loading checkpoint shards:  62%|   | 5/8 [00:01<00:00,  4.33it/s]Loading checkpoint shards:  75%|  | 6/8 [00:01<00:00,  4.49it/s]Loading checkpoint shards:  88%| | 7/8 [00:01<00:00,  4.65it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.95it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.22it/s]
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|        | 1/8 [00:00<00:03,  2.05it/s]Loading checkpoint shards:  25%|       | 2/8 [00:00<00:01,  3.07it/s]Loading checkpoint shards:  38%|      | 3/8 [00:00<00:01,  3.67it/s]Loading checkpoint shards:  50%|     | 4/8 [00:01<00:00,  4.04it/s]Loading checkpoint shards:  62%|   | 5/8 [00:01<00:00,  4.24it/s]Loading checkpoint shards:  75%|  | 6/8 [00:01<00:00,  4.46it/s]Loading checkpoint shards:  88%| | 7/8 [00:01<00:00,  4.60it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.90it/s]Loading checkpoint shards: 100%|| 8/8 [00:01<00:00,  4.18it/s]
_______________ MixedInt8T5Test.test_inference_with_keep_in_fp32 _______________

self = <test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32>

    def test_inference_with_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        import bitsandbytes as bnb
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
    
        # there was a bug with decoders - this test checks that it is fixed
        self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(0)
>       _ = model.generate(**encoded_input)

tests/transformer_tests/test_mixed_int8.py:525: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1739: in forward
    decoder_outputs = self.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  -1,    0,   -1,    0,   -1,    0,   -1,    1,    0,    0,    0,   -3,
            0,    0,   -9,    0,    0,    0,   -1,   -1,    0,    1,   -1,   -5,
            0,    0,    1,    1,    0,    0,    0,   -1,    0,    0,    1,   -1,
            0,    0,   -1,    1,    0,    0,   -2,    0,    0,    0,   22,   -1,
            1,    0,    0,    0,    1,   -1,    0,    0,    0,    1,    0,    0,
           -1,    1,    0,    0,    0,    0,    0,    0,   -2,    0,    1,    0,
            0,    0,    1,   -1,    0,    0,    0,    1,   -1,   -1,    0,    0,
            1,    1,    1,    1,    0,   -6,   -1,    1,    1,    0,    1,    0,
           -4,    1,    1,    0,    0,    0,    0,   -1,    0,    1,    0,    1,
            1,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,
            0,   -1,    0,  -14,   -1,   -2,   -1,    0,   -1,   -1,    0,    0,
            1,    0,    0,    1,    0,   -8,   -1,   -1,    0,   -1,    0,    0,
            0,    0,    0,   -1,    0,    6,    0,    0,    1,    0,    0,    0,
           -1,    0,   -1,    6,    0,    0,    0,    0,    1,   -1,    5,    0,
            0,    0,    1,    0,   -2,    0,    0,    0,    0,    1,    0,    0,
            0,    0,    0,    1,    0,   -1,    0,    0,    0,    0,   10,   -2,
            0,    2,    0,    0,    0,    0,    0,   -1,    0,   -1,    0,   -1,
            0,    1,    1,    0,   -1,    0,    0,   -1,    1,   -2,    0,    0,
            0,    1,    0,    1,    0,   -1,    1,   -1,   -1,    0,    0,    0,
            0,    1,    0,    1,   -1,    1,    0,    0,    1,    0,    0,    3,
            1,    0,    1,    1,    1,   -2,    0,    0,    3,    0,    1,    0,
            0,    0,    0,    0,    0,    1,   -1,    0,    1,    0,    0,   -3,
            0,    0,    0,    1,    1,    0,   -1,    0,    0,   -4,    0,   -1,
            0,    0,    1,    0,   -1,    0,    1,   -1,    0,    1,    0,    0,
            0,    0,    0,    0,    0,    0,    6,    2,    0,    0,    0,   -8,
            0,    0,    0,    1,   -1,    0,   -1,    0,   -6,    1,   -1,    0,
            7,    0,    0,    0,    0,    0,   19,    0,    0,    0,    0,    1,
            4,    0,   -1,    0,    0,   -1,    0,    1,   -1,    0,    0,    0,
            0,    1,    0,    0,    0,    1,    0,   -1,    0,   -1,    0,    0,
            0,    1,    0,    1,    0,    0,    0,    3,   -1,    0,    0,    0,
            0,    0,    0,    2,    0,    0,    1,   -2,   -1,    0,    0,    0,
            0,    0,   11,    0,    1,   -1,    0,    0,    0,    2,    0,   -1,
           -1,    0,   -2,   -1,   -1,    0,    1,    0,  -19,    0,    0,    0,
           -1,   -1,    0,   -1,    1,    1,    0,   -1,    1,    1,    0,    0,
            0,    0,    1,    0,    0,    2,    0,    0,   -1,    1,    0,    0,
           -1,    0,    0,   -1,    1,    0,    0,    0,    0,   -1,    0,    0,
            1,   -1,    0,    0,   -1,    0,    0,    0,   -1,   -1,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    4,    1,    0,    0,
            0,    0,    0,    0,    0,   -2,    0,    0,    0,    1,    0, -127,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,  -11,
            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,   -1,
           -1,    1,    0,    1,    0,    1,   -1,    0,    0,    1,    0,   -1,
            0,    1,   -1,    1,    0,    0,    1,   -1]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-14, -33,  37,  ...,   3,  30,   8],
        [ 34,  44,  56,  ...,  -1,  -4,  44],
        [-18,   3, -18,  ...,  -7,  -8,  15],
        ...,
        [ 12, -70,   9,  ...,  11,   9, -14],
        [-30,  14,   3,  ...,  -3, -11,  30],
        [ -2,  26, -28,  ...,  -3,  58, -24]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________ MixedInt8T5Test.test_inference_with_keep_in_fp32_serialized __________

self = <test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32_serialized>

    def test_inference_with_keep_in_fp32_serialized(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly on
        a serialized model.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        import bitsandbytes as bnb
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
    
        with tempfile.TemporaryDirectory() as tmp_dir:
            model.save_pretrained(tmp_dir)
    
            model = T5ForConditionalGeneration.from_pretrained(tmp_dir)
    
            # there was a bug with decoders - this test checks that it is fixed
            self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))
    
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(0)
>           _ = model.generate(**encoded_input)

tests/transformer_tests/test_mixed_int8.py:557: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1739: in forward
    decoder_outputs = self.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  -1,    0,   -1,    0,   -1,    0,   -1,    1,    0,    0,    0,   -3,
            0,    0,   -9,    0,    0,    0,   -1,   -1,    0,    1,   -1,   -5,
            0,    0,    1,    1,    0,    0,    0,   -1,    0,    0,    1,   -1,
            0,    0,   -1,    1,    0,    0,   -2,    0,    0,    0,   22,   -1,
            1,    0,    0,    0,    1,   -1,    0,    0,    0,    1,    0,    0,
           -1,    1,    0,    0,    0,    0,    0,    0,   -2,    0,    1,    0,
            0,    0,    1,   -1,    0,    0,    0,    1,   -1,   -1,    0,    0,
            1,    1,    1,    1,    0,   -6,   -1,    1,    1,    0,    1,    0,
           -4,    1,    1,    0,    0,    0,    0,   -1,    0,    1,    0,    1,
            1,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,
            0,   -1,    0,  -14,   -1,   -2,   -1,    0,   -1,   -1,    0,    0,
            1,    0,    0,    1,    0,   -8,   -1,   -1,    0,   -1,    0,    0,
            0,    0,    0,   -1,    0,    6,    0,    0,    1,    0,    0,    0,
           -1,    0,   -1,    6,    0,    0,    0,    0,    1,   -1,    5,    0,
            0,    0,    1,    0,   -2,    0,    0,    0,    0,    1,    0,    0,
            0,    0,    0,    1,    0,   -1,    0,    0,    0,    0,   10,   -2,
            0,    2,    0,    0,    0,    0,    0,   -1,    0,   -1,    0,   -1,
            0,    1,    1,    0,   -1,    0,    0,   -1,    1,   -2,    0,    0,
            0,    1,    0,    1,    0,   -1,    1,   -1,   -1,    0,    0,    0,
            0,    1,    0,    1,   -1,    1,    0,    0,    1,    0,    0,    3,
            1,    0,    1,    1,    1,   -2,    0,    0,    3,    0,    1,    0,
            0,    0,    0,    0,    0,    1,   -1,    0,    1,    0,    0,   -3,
            0,    0,    0,    1,    1,    0,   -1,    0,    0,   -4,    0,   -1,
            0,    0,    1,    0,   -1,    0,    1,   -1,    0,    1,    0,    0,
            0,    0,    0,    0,    0,    0,    6,    2,    0,    0,    0,   -8,
            0,    0,    0,    1,   -1,    0,   -1,    0,   -6,    1,   -1,    0,
            7,    0,    0,    0,    0,    0,   19,    0,    0,    0,    0,    1,
            4,    0,   -1,    0,    0,   -1,    0,    1,   -1,    0,    0,    0,
            0,    1,    0,    0,    0,    1,    0,   -1,    0,   -1,    0,    0,
            0,    1,    0,    1,    0,    0,    0,    3,   -1,    0,    0,    0,
            0,    0,    0,    2,    0,    0,    1,   -2,   -1,    0,    0,    0,
            0,    0,   11,    0,    1,   -1,    0,    0,    0,    2,    0,   -1,
           -1,    0,   -2,   -1,   -1,    0,    1,    0,  -19,    0,    0,    0,
           -1,   -1,    0,   -1,    1,    1,    0,   -1,    1,    1,    0,    0,
            0,    0,    1,    0,    0,    2,    0,    0,   -1,    1,    0,    0,
           -1,    0,    0,   -1,    1,    0,    0,    0,    0,   -1,    0,    0,
            1,   -1,    0,    0,   -1,    0,    0,    0,   -1,   -1,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    4,    1,    0,    0,
            0,    0,    0,    0,    0,   -2,    0,    0,    0,    1,    0, -127,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,  -11,
            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,   -1,
           -1,    1,    0,    1,    0,    1,   -1,    0,    0,    1,    0,   -1,
            0,    1,   -1,    1,    0,    0,    1,   -1]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-14, -33,  37,  ...,   3,  30,   8],
        [ 34,  44,  56,  ...,  -1,  -4,  44],
        [-18,   3, -18,  ...,  -7,  -8,  15],
        ...,
        [ 12, -70,   9,  ...,  11,   9, -14],
        [-30,  14,   3,  ...,  -3, -11,  30],
        [ -2,  26, -28,  ...,  -3,  58, -24]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
_____________ MixedInt8T5Test.test_inference_without_keep_in_fp32 ______________

self = <test_mixed_int8.MixedInt8T5Test testMethod=test_inference_without_keep_in_fp32>

    def test_inference_without_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        from transformers import T5ForConditionalGeneration
    
        modules = T5ForConditionalGeneration._keep_in_fp32_modules
        T5ForConditionalGeneration._keep_in_fp32_modules = None
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(0)
>       _ = model.generate(**encoded_input)

tests/transformer_tests/test_mixed_int8.py:498: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1739: in forward
    decoder_outputs = self.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  -1,    0,   -1,    0,   -1,    0,   -1,    1,    0,    0,    0,   -3,
            0,    0,   -9,    0,    0,    0,   -1,   -1,    0,    1,   -1,   -5,
            0,    0,    1,    1,    0,    0,    0,   -1,    0,    0,    1,   -1,
            0,    0,   -1,    1,    0,    0,   -2,    0,    0,    0,   22,   -1,
            1,    0,    0,    0,    1,   -1,    0,    0,    0,    1,    0,    0,
           -1,    1,    0,    0,    0,    0,    0,    0,   -2,    0,    1,    0,
            0,    0,    1,   -1,    0,    0,    0,    1,   -1,   -1,    0,    0,
            1,    1,    1,    1,    0,   -6,   -1,    1,    1,    0,    1,    0,
           -4,    1,    1,    0,    0,    0,    0,   -1,    0,    1,    0,    1,
            1,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,
            0,   -1,    0,  -14,   -1,   -2,   -1,    0,   -1,   -1,    0,    0,
            1,    0,    0,    1,    0,   -8,   -1,   -1,    0,   -1,    0,    0,
            0,    0,    0,   -1,    0,    6,    0,    0,    1,    0,    0,    0,
           -1,    0,   -1,    6,    0,    0,    0,    0,    1,   -1,    5,    0,
            0,    0,    1,    0,   -2,    0,    0,    0,    0,    1,    0,    0,
            0,    0,    0,    1,    0,   -1,    0,    0,    0,    0,   10,   -2,
            0,    2,    0,    0,    0,    0,    0,   -1,    0,   -1,    0,   -1,
            0,    1,    1,    0,   -1,    0,    0,   -1,    1,   -2,    0,    0,
            0,    1,    0,    1,    0,   -1,    1,   -1,   -1,    0,    0,    0,
            0,    1,    0,    1,   -1,    1,    0,    0,    1,    0,    0,    3,
            1,    0,    1,    1,    1,   -2,    0,    0,    3,    0,    1,    0,
            0,    0,    0,    0,    0,    1,   -1,    0,    1,    0,    0,   -3,
            0,    0,    0,    1,    1,    0,   -1,    0,    0,   -4,    0,   -1,
            0,    0,    1,    0,   -1,    0,    1,   -1,    0,    1,    0,    0,
            0,    0,    0,    0,    0,    0,    6,    2,    0,    0,    0,   -8,
            0,    0,    0,    1,   -1,    0,   -1,    0,   -6,    1,   -1,    0,
            7,    0,    0,    0,    0,    0,   19,    0,    0,    0,    0,    1,
            4,    0,   -1,    0,    0,   -1,    0,    1,   -1,    0,    0,    0,
            0,    1,    0,    0,    0,    1,    0,   -1,    0,   -1,    0,    0,
            0,    1,    0,    1,    0,    0,    0,    3,   -1,    0,    0,    0,
            0,    0,    0,    2,    0,    0,    1,   -2,   -1,    0,    0,    0,
            0,    0,   11,    0,    1,   -1,    0,    0,    0,    2,    0,   -1,
           -1,    0,   -2,   -1,   -1,    0,    1,    0,  -19,    0,    0,    0,
           -1,   -1,    0,   -1,    1,    1,    0,   -1,    1,    1,    0,    0,
            0,    0,    1,    0,    0,    2,    0,    0,   -1,    1,    0,    0,
           -1,    0,    0,   -1,    1,    0,    0,    0,    0,   -1,    0,    0,
            1,   -1,    0,    0,   -1,    0,    0,    0,   -1,   -1,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    4,    1,    0,    0,
            0,    0,    0,    0,    0,   -2,    0,    0,    0,    1,    0, -127,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,  -11,
            1,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,   -1,
           -1,    1,    0,    1,    0,    1,   -1,    0,    0,    1,    0,   -1,
            0,    1,   -1,    1,    0,    0,    1,   -1]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-14, -33,  37,  ...,   3,  30,   8],
        [ 34,  44,  56,  ...,  -1,  -4,  44],
        [-18,   3, -18,  ...,  -7,  -8,  15],
        ...,
        [ 12, -70,   9,  ...,  11,   9, -14],
        [-30,  14,   3,  ...,  -3, -11,  30],
        [ -2,  26, -28,  ...,  -3,  58, -24]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
A: torch.Size([1, 512]), B: torch.Size([512, 512]), C: (1, 512); (lda, ldb, ldc): (c_int(1), c_int(512), c_int(1)); (m, n, k): (c_int(1), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_____________________ MixedInt8TestPipeline.test_pipeline ______________________

self = <test_mixed_int8.MixedInt8TestPipeline testMethod=test_pipeline>

    def test_pipeline(self):
        r"""
        The aim of this test is to verify that the mixed int8 is compatible with `pipeline` from transformers. Since
        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything
        on pipline.
        """
        # self._clear_cuda_cache()
        self.pipe = pipeline(
            "text-generation",
            model=self.model_name,
            model_kwargs={"device_map": "auto", "load_in_8bit": True},
            max_new_tokens=self.MAX_NEW_TOKENS,
        )
    
        # Real second forward pass
>       pipeline_output = self.pipe(self.input_text)

tests/transformer_tests/test_mixed_int8.py:646: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py:262: in __call__
    return super().__call__(text_inputs, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1254: in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1261: in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1161: in forward
    model_outputs = self._forward(model_inputs, **forward_params)
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py:351: in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ 17,  10,  27,  ..., -14,   1,  33]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________________ MixedInt8TestMultiGpu.test_multi_gpu_loading _________________

self = <test_mixed_int8.MixedInt8TestMultiGpu testMethod=test_multi_gpu_loading>

    def test_multi_gpu_loading(self):
        r"""
        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.
        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice
        """
    
        model_parallel = AutoModelForCausalLM.from_pretrained(
            self.model_name, load_in_8bit=True, device_map="balanced"
        )
    
        # Check correct device map
>       self.assertEqual(set(model_parallel.hf_device_map.values()), {0, 1})
E       AssertionError: Items in the first set but not the second:
E       15
E       Items in the second set but not the first:
E       0
E       1

tests/transformer_tests/test_mixed_int8.py:666: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_______ MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map ________

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map>

    def test_cpu_gpu_disk_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
            model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                quantization_config=bnb_config,
                offload_folder=tmpdirname,
            )
    
            # Check that the model has been correctly set on device 0, 1, and `cpu`.
            self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu", "disk"})
    
>           self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:792: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
____ MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map_kwargs ____

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map_kwargs>

    def test_cpu_gpu_disk_loading_custom_device_map_kwargs(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map - using the kwargs directly instead of the quantization config
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
            model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                offload_folder=tmpdirname,
            )
    
            # Check that the model has been correctly set on device 0, 1, and `cpu`.
            self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu", "disk"})
    
>           self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:819: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [ 41,  63,  -9,  ..., -61,  20, -20],
        [ 10, -16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the disk and cpu.
__________ MixedInt8TestCpuGpu.test_cpu_gpu_loading_custom_device_map __________

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_custom_device_map>

    def test_cpu_gpu_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time the device map is more organized than the test above and uses the abstraction
        `transformer.h` to encapsulate all the decoder layers.
        """
        device_map = {
            "transformer.word_embeddings": "cpu",
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": "cpu",
            "transformer.h": 0,
            "transformer.ln_f": 1,
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
        # Load model
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )
    
        # Check that the model has been correctly set on device 0, 1, and `cpu`.
        self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu"})
    
>       self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:765: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:0',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ...,  27,  21, -10],
        ...,
        [  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0]], device='cuda:0',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
__________ MixedInt8TestCpuGpu.test_cpu_gpu_loading_random_device_map __________

self = <test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_random_device_map>

    def test_cpu_gpu_loading_random_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a random `device_map`.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": 0,
            "lm_head": 0,
            "transformer.h.0": "cpu",
            "transformer.h.1": "cpu",
            "transformer.h.2": 0,
            "transformer.h.3": 0,
            "transformer.h.4": 0,
            "transformer.h.5": 0,
            "transformer.h.6": 0,
            "transformer.h.7": 0,
            "transformer.h.8": 0,
            "transformer.h.9": 1,
            "transformer.h.10": 0,
            "transformer.h.11": 1,
            "transformer.h.12": 0,
            "transformer.h.13": 0,
            "transformer.h.14": 1,
            "transformer.h.15": 0,
            "transformer.h.16": 0,
            "transformer.h.17": 1,
            "transformer.h.18": 1,
            "transformer.h.19": 0,
            "transformer.h.20": 1,
            "transformer.h.21": 1,
            "transformer.h.22": 0,
            "transformer.h.23": 0,
            "transformer.ln_f": 1,
        }
    
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )
    
        # Check that the model has been correctly set on device 0, 1, and `cpu`.
        self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu"})
    
>       self.check_inference_correctness(model_8bit)

tests/transformer_tests/test_mixed_int8.py:738: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/transformer_tests/test_mixed_int8.py:686: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:848: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:712: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:400: in forward
    attn_outputs = self.self_attention(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py:251: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -6,  -6,  22,  ...,  21,  12,  -4],
        [ -1, -12,   7,  ...,  -9,  18,  15],
        [ -4,  -4, -15,  ...,   4, -23, -15],
        [ 19,  16,  20,  ...,  -7,  11,  -9]], device='cuda:0',
       dtype=torch.int8)
B = tensor([[ -8,  18,   7,  ..., -26,  -7, -43],
        [-20, -10, -11,  ...,  32, -53, -23],
        [ 30, -35, -45,  ..., -26, -52, -15],
        ...,
        [-88,   5, -13,  ...,  27, -15,  36],
        [-41,  21, -21,  ..., -20,  26, -16],
        [-36, -21,  24,  ..., -40, -26,  22]], device='cuda:0',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
_____________________ MixedInt8TestTraining.test_training ______________________

self = <test_mixed_int8.MixedInt8TestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version("bitsandbytes")) < version.parse("0.37.0"):
            self.skipTest(reason="This test requires bitsandbytes>=0.37.0")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)
    
        self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})
    
        for param in model.parameters():
            param.requires_grad = False  # freeze the model - train adapters later
            if param.ndim == 1:
                # cast the small parameters (e.g. layernorm) to fp32 for stability
                param.data = param.data.to(torch.float32)
    
        # Step 2: add adapters
        for _, module in model.named_modules():
            if "OPTAttention" in repr(type(module)):
                module.q_proj = LoRALayer(module.q_proj, rank=16)
                module.k_proj = LoRALayer(module.k_proj, rank=16)
                module.v_proj = LoRALayer(module.v_proj, rank=16)
    
        # Step 3: dummy batch
        batch = self.tokenizer("Test batch ", return_tensors="pt").to(0)
    
        # Step 4: Check if the gradient is not None
        with torch.cuda.amp.autocast():
>           out = model.forward(**batch)

tests/transformer_tests/test_mixed_int8.py:854: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py:1011: in forward
    outputs = self.model.decoder(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py:729: in forward
    inputs_embeds = self.project_in(inputs_embeds)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[-43, -30, -60,  ..., -22,  -4,  32],
        [ 65,   9,  45,  ...,  29,  66, -59],
        [-17, -17,  35,  ..., -11, -64, -55],
        [-23,   5, -40,  ..., -74, -11,  -8]], device='cuda:0',
       dtype=torch.int8)
B = tensor([[114, -71, -27,  ..., -73, -47,  66],
        [ 70,  95, -20,  ...,  15,  47, -15],
        [-86,  -4,   8,  ...,  62, -64, -10],
        ...,
        [ 87,  95,  55,  ...,  33,  16,  -9],
        [ 44,  11, -47,  ..., -43, -13, -47],
        [ 57, -15, -66,  ...,  42,  40, -64]], device='cuda:0',
       dtype=torch.int8)
SA = (torch.Size([4, 512]), 'col'), SB = (torch.Size([1024, 512]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)
Sout = ((4, 1024), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 512]), B: torch.Size([1024, 512]), C: (4, 1024); (lda, ldb, ldc): (c_int(4), c_int(1024), c_int(4)); (m, n, k): (c_int(4), c_int(1024), c_int(512))
A: torch.Size([4, 512]), B: torch.Size([1024, 512]), C: (4, 1024); (lda, ldb, ldc): (c_int(4), c_int(1024), c_int(4)); (m, n, k): (c_int(4), c_int(1024), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
_________________ MixedInt8GPT2Test.test_fp32_int8_conversion __________________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_fp32_int8_conversion>

    def test_fp32_int8_conversion(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        """
        model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small", load_in_8bit=True, device_map="auto")
>       self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.float32)
E       AssertionError: False is not true

tests/transformer_tests/test_mixed_int8.py:365: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
___________________ MixedInt8GPT2Test.test_generate_quality ____________________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality>

    def test_generate_quality(self):
        r"""
        Test the generation quality of the quantized model and see that we are matching the expected output.
        Given that we are operating on small numbers + the testing model is relatively small, we might not get
        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.
        """
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = self.model_8bit.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
________________ MixedInt8GPT2Test.test_generate_quality_config ________________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality_config>

    def test_generate_quality_config(self):
        r"""
        Test that loading the model with the config is equivalent
        """
        bnb_config = BitsAndBytesConfig()
        bnb_config.load_in_8bit = True
    
        model_8bit_from_config = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit_from_config.generate(
            input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10
        )

tests/transformer_tests/test_mixed_int8.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
_________________ MixedInt8GPT2Test.test_int8_from_pretrained __________________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_from_pretrained>

    def test_int8_from_pretrained(self):
        r"""
        Test whether loading a 8bit model from the Hub works as expected
        """
        from bitsandbytes.nn import Int8Params
    
        model_id = "ybelkada/gpt2-xl-8bit"
    
        model = AutoModelForCausalLM.from_pretrained(model_id)
    
        linear = get_some_linear_layer(model)
        self.assertTrue(linear.weight.__class__ == Int8Params)
        self.assertTrue(hasattr(linear.weight, "SCB"))
    
        # generate
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:890: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
__________________ MixedInt8GPT2Test.test_int8_serialization ___________________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization>

    def test_int8_serialization(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
_____________ MixedInt8GPT2Test.test_int8_serialization_regression _____________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_regression>

    def test_int8_serialization_regression(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - using not safetensors
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, safe_serialization=False)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:413: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
______________ MixedInt8GPT2Test.test_int8_serialization_sharded _______________

self = <test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_sharded>

    def test_int8_serialization_sharded(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - sharded version.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, max_shard_size="200MB")
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname)
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(input_ids=encoded_input["input_ids"].to(0), max_new_tokens=10)

tests/transformer_tests/test_mixed_int8.py:438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1989: in generate
    result = self._sample(
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2932: in _sample
    outputs = self(**model_inputs, return_dict=True)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7feca07f5810>
A = tensor([[ -7,   0,  -3,  ...,  72, -33,   4]], device='cuda:15',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ..., -18,  35,  22],
        ...,
        [-30,  28,  19,  ..., -33,  32,  97],
        [-25,  24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:15',
       dtype=torch.int8)
SA = (torch.Size([1, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:15', dtype=torch.int32)
Sout = ((1, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
A: torch.Size([1, 1600]), B: torch.Size([4800, 1600]), C: (1, 4800); (lda, ldb, ldc): (c_int(1), c_int(4800), c_int(1)); (m, n, k): (c_int(1), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|         | 1/9 [00:00<00:01,  4.34it/s]Loading checkpoint shards:  22%|       | 2/9 [00:00<00:01,  4.22it/s]Loading checkpoint shards:  33%|      | 3/9 [00:00<00:01,  4.21it/s]Loading checkpoint shards:  44%|     | 4/9 [00:00<00:01,  4.20it/s]Loading checkpoint shards:  56%|    | 5/9 [00:01<00:00,  4.23it/s]Loading checkpoint shards:  67%|   | 6/9 [00:01<00:00,  4.25it/s]Loading checkpoint shards:  78%|  | 7/9 [00:01<00:00,  4.27it/s]Loading checkpoint shards:  89%| | 8/9 [00:01<00:00,  4.28it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.52it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.34it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|         | 1/9 [00:00<00:01,  4.27it/s]Loading checkpoint shards:  22%|       | 2/9 [00:00<00:01,  4.19it/s]Loading checkpoint shards:  33%|      | 3/9 [00:00<00:01,  4.20it/s]Loading checkpoint shards:  44%|     | 4/9 [00:00<00:01,  4.17it/s]Loading checkpoint shards:  56%|    | 5/9 [00:01<00:00,  4.17it/s]Loading checkpoint shards:  67%|   | 6/9 [00:01<00:00,  4.18it/s]Loading checkpoint shards:  78%|  | 7/9 [00:01<00:00,  4.15it/s]Loading checkpoint shards:  89%| | 8/9 [00:01<00:00,  4.10it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.34it/s]Loading checkpoint shards: 100%|| 9/9 [00:02<00:00,  4.23it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
=============================== warnings summary ===============================
../../usr/local/lib/python3.10/dist-packages/transformers/testing_utils.py:1078
../../usr/local/lib/python3.10/dist-packages/transformers/testing_utils.py:1078
  /usr/local/lib/python3.10/dist-packages/transformers/testing_utils.py:1078: PytestUnknownMarkWarning: Unknown pytest.mark.bitsandbytes - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    return pytest.mark.bitsandbytes(test_case)

tests/transformer_tests/test_4bit.py: 10 warnings
  /home/bitsandbytes/bitsandbytes/nn/modules.py:437: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.
    warnings.warn(

tests/transformer_tests/test_4bit.py: 8 warnings
tests/transformer_tests/test_mixed_int8.py: 4 warnings
  /usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
    warnings.warn(

tests/transformer_tests/test_4bit.py::Bnb4BitT5Test::test_inference_with_keep_in_fp32
tests/transformer_tests/test_4bit.py::Bnb4BitT5Test::test_inference_without_keep_in_fp32
  /home/bitsandbytes/bitsandbytes/nn/modules.py:432: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference.
    warnings.warn(

tests/transformer_tests/test_4bit.py::Bnb4BitTestTraining::test_training
tests/transformer_tests/test_4bit.py::Bnb4BitTestTraining::test_training
  /home/bitsandbytes/tests/transformer_tests/test_4bit.py:523: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
    with torch.cuda.amp.autocast():

tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality_config
tests/transformer_tests/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality_config
  /home/bitsandbytes/bitsandbytes/autograd/_functions.py:581: UserWarning: Some matrices hidden dimension is not a multiple of 128 and efficient inference kernels are not supported for these (slow). Matrix input size found: torch.Size([1, 1, 1600])
    warn(

tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code
  /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/configuration_mpt.py:90: DeprecationWarning: verbose argument for MPTConfig is now ignored and will be removed. Use python_log_level instead.
    warnings.warn(DeprecationWarning('verbose argument for MPTConfig is now ignored and will be removed. Use python_log_level instead.'))

tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code
  /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb` to `False.`
    warnings.warn(f'alibi is turned on, setting `learned_pos_emb` to `False.`')

tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_regression
tests/transformer_tests/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_regression
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_regression
tests/transformer_tests/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_regression
  /usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
    warnings.warn(warning_msg)

tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_custom_device_map
tests/transformer_tests/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_custom_device_map
  /usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1850: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.
    warnings.warn(

tests/transformer_tests/test_mixed_int8.py::MixedInt8TestTraining::test_training
  /home/bitsandbytes/tests/transformer_tests/test_mixed_int8.py:853: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
    with torch.cuda.amp.autocast():

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
====== 37 failed, 51 passed, 50 warnings, 37 errors in 1031.28s (0:17:11) ======
error detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detectederror detected